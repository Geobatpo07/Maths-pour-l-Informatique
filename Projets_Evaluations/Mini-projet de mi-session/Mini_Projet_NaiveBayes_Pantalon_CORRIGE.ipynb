{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a1985b9",
   "metadata": {},
   "source": [
    "\n",
    "# ✅ Corrigé — Mini‑projet : Classifieur Bayésien Naïf (Bernoulli)\n",
    "\n",
    "**Cours :** Mathématiques pour l'informatique — FSGA / Université Quisqueya  \n",
    "**Enseignant :** Geovany Batista Polo LAGUERRE — S1 — 2025–2026\n",
    "\n",
    "Ce notebook fournit une **solution de référence** : comptages, lissage de Laplace, prédictions, comparaison `alpha=0` vs `alpha=1`, et une visualisation finale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124cff0b",
   "metadata": {},
   "source": [
    "## Données jouet (recréation du CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e7050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv, pandas as pd\n",
    "from pathlib import Path\n",
    "csv_path = Path(\"/mnt/data/train_pantalon.csv\")\n",
    "rows = [\n",
    "    {\"id\":1, \"pas_cher\":1, \"anglais\":0, \"achat\":\"OUI\"},\n",
    "    {\"id\":2, \"pas_cher\":0, \"anglais\":1, \"achat\":\"NON\"},\n",
    "    {\"id\":3, \"pas_cher\":0, \"anglais\":1, \"achat\":\"NON\"},\n",
    "    {\"id\":4, \"pas_cher\":0, \"anglais\":1, \"achat\":\"NON\"},\n",
    "    {\"id\":5, \"pas_cher\":1, \"anglais\":0, \"achat\":\"NON\"},\n",
    "    {\"id\":6, \"pas_cher\":1, \"anglais\":1, \"achat\":\"OUI\"},\n",
    "    {\"id\":7, \"pas_cher\":1, \"anglais\":0, \"achat\":\"OUI\"},\n",
    "    {\"id\":8, \"pas_cher\":1, \"anglais\":0, \"achat\":\"OUI\"},\n",
    "]\n",
    "csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"id\",\"pas_cher\",\"anglais\",\"achat\"])\n",
    "    w.writeheader(); w.writerows(rows)\n",
    "df = pd.read_csv(csv_path)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a46a8",
   "metadata": {},
   "source": [
    "## Implémentation (avec lissage de Laplace implémenté)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97cb26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "from collections import Counter, defaultdict\n",
    "from math import log, exp\n",
    "import pandas as pd\n",
    "\n",
    "class NaiveBayesBernoulli:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = float(alpha)\n",
    "        self.classes_ = []\n",
    "        self.features_ = []\n",
    "        self.class_counts_ = Counter()\n",
    "        self.feature_counts_ = defaultdict(lambda: Counter())\n",
    "        self.n_ = 0\n",
    "\n",
    "    def fit(self, X_list, y_list):\n",
    "        self.n_ = len(y_list)\n",
    "        self.classes_ = sorted(set(y_list))\n",
    "        feat = set()\n",
    "        for X in X_list:\n",
    "            feat |= set(X.keys())\n",
    "        self.features_ = sorted(feat)\n",
    "        self.class_counts_.clear()\n",
    "        self.feature_counts_.clear()\n",
    "        for X, y in zip(X_list, y_list):\n",
    "            self.class_counts_[y] += 1\n",
    "            for f in self.features_:\n",
    "                v = int(X.get(f, 0))\n",
    "                self.feature_counts_[y][(f, v)] += 1\n",
    "        return self\n",
    "\n",
    "    def _p_class(self, c):\n",
    "        return self.class_counts_[c] / self.n_\n",
    "\n",
    "    def _p_feat_given_class(self, feat, val, c):\n",
    "        c1 = self.feature_counts_[c][(feat, 1)]\n",
    "        c0 = self.feature_counts_[c][(feat, 0)]\n",
    "        tot = c1 + c0\n",
    "        num = (c1 + self.alpha) if val == 1 else (c0 + self.alpha)\n",
    "        den = tot + 2*self.alpha\n",
    "        return num / den\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        scores = {}\n",
    "        for c in self.classes_:\n",
    "            s = log(self._p_class(c))\n",
    "            for f in self.features_:\n",
    "                v = int(X.get(f, 0))\n",
    "                s += log(self._p_feat_given_class(f, v, c))\n",
    "            scores[c] = s\n",
    "        m = max(scores.values())\n",
    "        exps = {c: exp(v - m) for c, v in scores.items()}\n",
    "        Z = sum(exps.values())\n",
    "        return {c: exps[c]/Z for c in self.classes_}\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return max(proba, key=proba.get)\n",
    "\n",
    "def load_csv_binary(path, feature_names=(\"pas_cher\",\"anglais\")):\n",
    "    X_list, y_list = [], []\n",
    "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        for row in csv.DictReader(f):\n",
    "            X = {feat: int(row[feat]) for feat in feature_names}\n",
    "            y = row[\"achat\"].strip()\n",
    "            X_list.append(X); y_list.append(y)\n",
    "    return X_list, y_list\n",
    "\n",
    "def pretty_counts_df(nb: NaiveBayesBernoulli):\n",
    "    rows = []\n",
    "    for c in nb.classes_:\n",
    "        for f in nb.features_:\n",
    "            c1 = nb.feature_counts_[c][(f,1)]\n",
    "            c0 = nb.feature_counts_[c][(f,0)]\n",
    "            tot = c1 + c0\n",
    "            p1 = (c1 + nb.alpha) / (tot + 2*nb.alpha)\n",
    "            p0 = (c0 + nb.alpha) / (tot + 2*nb.alpha)\n",
    "            rows.append({\n",
    "                \"classe\": c, \"feature\": f,\n",
    "                \"#1\": c1, \"#0\": c0, \"total\": tot,\n",
    "                \"p(1|classe)\": round(p1,3), \"p(0|classe)\": round(p0,3)\n",
    "            })\n",
    "    priors = {c: nb.class_counts_[c]/nb.n_ for c in nb.classes_}\n",
    "    return pd.DataFrame(rows), priors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e9029",
   "metadata": {},
   "source": [
    "## Entraînement et comptages (alpha = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb831766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, y_train = load_csv_binary(\"/mnt/data/train_pantalon.csv\", feature_names=(\"pas_cher\",\"anglais\"))\n",
    "nb1 = NaiveBayesBernoulli(alpha=1.0).fit(X_train, y_train)\n",
    "df_counts, priors = pretty_counts_df(nb1)\n",
    "print(\"Priors (p(c)) :\", {k:round(v,3) for k,v in priors.items()})\n",
    "df_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46140f24",
   "metadata": {},
   "source": [
    "## Prédictions — comparaison alpha=1.0 vs alpha=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0013de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "tests = [\n",
    "    {\"pas_cher\":1, \"anglais\":1},\n",
    "    {\"pas_cher\":1, \"anglais\":0},\n",
    "    {\"pas_cher\":0, \"anglais\":1},\n",
    "    {\"pas_cher\":0, \"anglais\":0},\n",
    "]\n",
    "\n",
    "def run_table(alpha):\n",
    "    nb = NaiveBayesBernoulli(alpha=alpha).fit(X_train, y_train)\n",
    "    rows = []\n",
    "    for x in tests:\n",
    "        proba = nb.predict_proba(x)\n",
    "        yhat  = nb.predict(x)\n",
    "        rows.append({\n",
    "            \"alpha\": alpha,\n",
    "            \"pas_cher\": x[\"pas_cher\"],\n",
    "            \"anglais\": x[\"anglais\"],\n",
    "            \"p(OUI)\": round(proba.get(\"OUI\",0.0), 3),\n",
    "            \"p(NON)\": round(proba.get(\"NON\",0.0), 3),\n",
    "            \"pred\": yhat\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "tab = pd.concat([run_table(1.0), run_table(0.0)], ignore_index=True)\n",
    "tab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e47a6",
   "metadata": {},
   "source": [
    "\n",
    "### Commentaire rapide\n",
    "- Le prior est équilibré ici **p(OUI)=p(NON)=0.5** (4/8 chacun).  \n",
    "- Avec lissage `alpha=1` : les probabilités conditionnelles sont **tirées vers 0.5**, évitant les zéros (stabilisation).  \n",
    "- Pour `x=[1,1]`, la prédiction est **OUI** dans les deux cas, mais les scores lissés sont moins extrêmes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04274aee",
   "metadata": {},
   "source": [
    "## Visualisation : carte des scores p(OUI | pas_cher, anglais) (alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9255387",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "nb = NaiveBayesBernoulli(alpha=1.0).fit(X_train, y_train)\n",
    "grid = [(a,b) for a in [0,1] for b in [0,1]]\n",
    "scores = [ nb.predict_proba({\"pas_cher\":a,\"anglais\":b})[\"OUI\"] for a,b in grid ]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "im = ax.imshow(np.array(scores).reshape(2,2), vmin=0, vmax=1, origin=\"lower\")\n",
    "ax.set_xticks([0,1]); ax.set_xticklabels([\"anglais=0\",\"anglais=1\"])\n",
    "ax.set_yticks([0,1]); ax.set_yticklabels([\"pas_cher=0\",\"pas_cher=1\"])\n",
    "for i,(a,b) in enumerate(grid):\n",
    "    ax.text(b, a, f\"{scores[i]:.2f}\", ha=\"center\", va=\"center\", color=\"w\")\n",
    "ax.set_title(\"p(OUI | pas_cher, anglais) — alpha=1\")\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
