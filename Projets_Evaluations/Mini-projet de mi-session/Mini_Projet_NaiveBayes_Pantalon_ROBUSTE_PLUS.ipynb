{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55efdaa",
   "metadata": {},
   "source": [
    "# Mini‑projet - Classifieur Bayésien Naïf (Bernoulli)\n",
    "**Mathématiques pour Informaticiens - FSGA / Université Quisqueya**\n",
    "\n",
    "**Enseignant : Geovany Batista Polo LAGUERRE | Data Scientist**\n",
    "\n",
    "**Semestre 1 - 2025–2026**\n",
    "\n",
    "Ce notebook contient **tout le nécessaire** pour votre mini‑projet :\n",
    "- classe `NaiveBayesBernoulli` (avec **lissage de Laplace** déjà implémenté),\n",
    "- fonctions utilitaires pour lire les données,\n",
    "- **pipeline robuste** de prédiction (texte libre ou vecteurs binaires) avec `safe_predict`,\n",
    "- TODO structurés pour vous guider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac8575",
   "metadata": {},
   "source": [
    "## Règles & rendu\n",
    "- Travail **individuel**. Autorisés : `csv`, `math`, `collections`, `itertools`, `random`, `matplotlib`. **Interdit :** `scikit-learn`.\n",
    "- Rendez ce notebook **exécuté** (toutes les sorties présentes).  \n",
    "- Nommez le fichier : `NOM_Prenom_NaiveBayes_Pantalon.ipynb`.\n",
    "\n",
    "### Barème (rappel)\n",
    "- Implémentation correcte (utilisation de la classe + pipeline) - 35 pts\n",
    "- Lissage de Laplace **utilisé** et **interprété** - 15 pts\n",
    "- Dénombrements & fréquences affichés - 15 pts\n",
    "- Démo et cas tests pertinents - 15 pts\n",
    "- Qualité du code & commentaires - 10 pts\n",
    "- Analyse & limites/pistes - 10 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970078e4",
   "metadata": {},
   "source": [
    "## Classe fournie : `NaiveBayesBernoulli` (lissage de **Laplace** déjà implémenté)\n",
    "- **À VOUS** d’**utiliser** cette classe dans le pipeline (chargement, fit, prédiction, affichage des comptes, etc.).\n",
    "- Vous pouvez ajouter de **nouvelles features binaires** (facultatif)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a807f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter, defaultdict\n",
    "from math import log, exp\n",
    "\n",
    "class NaiveBayesBernoulli:\n",
    "    \"\"\"\n",
    "    Classifieur Bayésien Naïf (modèle Bernoulli binaire)\n",
    "    - X : dict binaire {'pas_cher':0/1, 'anglais':0/1, ...}\n",
    "    - y : 'OUI' / 'NON'\n",
    "    - Lissage de Laplace (alpha) : déjà implémenté.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = float(alpha)\n",
    "        self.classes_ = []\n",
    "        self.features_ = []\n",
    "        self.class_counts_ = Counter()\n",
    "        self.feature_counts_ = defaultdict(lambda: Counter())\n",
    "        self.n_ = 0\n",
    "\n",
    "    def fit(self, X_list, y_list):\n",
    "        self.n_ = len(y_list)\n",
    "        self.classes_ = sorted(set(y_list))\n",
    "        feat = set()\n",
    "        for X in X_list:\n",
    "            feat |= set(X.keys())\n",
    "        self.features_ = sorted(feat)\n",
    "        self.class_counts_.clear()\n",
    "        self.feature_counts_.clear()\n",
    "        for X, y in zip(X_list, y_list):\n",
    "            self.class_counts_[y] += 1\n",
    "            for f in self.features_:\n",
    "                v = int(X.get(f, 0))\n",
    "                self.feature_counts_[y][(f, v)] += 1\n",
    "        return self\n",
    "\n",
    "    def _p_class(self, c):\n",
    "        return self.class_counts_[c] / self.n_\n",
    "\n",
    "    def _p_feat_given_class(self, feat, val, c):\n",
    "        c1 = self.feature_counts_[c][(feat, 1)]\n",
    "        c0 = self.feature_counts_[c][(feat, 0)]\n",
    "        tot = c1 + c0\n",
    "        num = (c1 + self.alpha) if val == 1 else (c0 + self.alpha)\n",
    "        den = tot + 2*self.alpha\n",
    "        return num / den\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        scores = {}\n",
    "        for c in self.classes_:\n",
    "            s = log(self._p_class(c))\n",
    "            for f in self.features_:\n",
    "                v = int(X.get(f, 0))\n",
    "                s += log(self._p_feat_given_class(f, v, c))\n",
    "            scores[c] = s\n",
    "        m = max(scores.values())\n",
    "        exps = {c: exp(v - m) for c, v in scores.items()}\n",
    "        Z = sum(exps.values())\n",
    "        return {c: exps[c]/Z for c in self.classes_}\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return max(proba, key=proba.get)\n",
    "\n",
    "def load_csv_binary(path, feature_names=(\"pas_cher\",\"anglais\")):\n",
    "    X_list, y_list = [], []\n",
    "    with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        for row in csv.DictReader(f):\n",
    "            X = {feat: int(row[feat]) for feat in feature_names}\n",
    "            y = row[\"achat\"].strip()\n",
    "            X_list.append(X); y_list.append(y)\n",
    "    return X_list, y_list\n",
    "\n",
    "def pretty_counts(nb: NaiveBayesBernoulli):\n",
    "    print(\"Classes :\", nb.classes_)\n",
    "    print(\"Features:\", nb.features_)\n",
    "    for c in nb.classes_:\n",
    "        print(f\"\\nClasse {c} (count={nb.class_counts_[c]})\")\n",
    "        for f in nb.features_:\n",
    "            c1 = nb.feature_counts_[c][(f,1)]\n",
    "            c0 = nb.feature_counts_[c][(f,0)]\n",
    "            tot = c1 + c0\n",
    "            p1 = (c1 + nb.alpha) / (tot + 2*nb.alpha)\n",
    "            p0 = (c0 + nb.alpha) / (tot + 2*nb.alpha)\n",
    "            print(f\"  {f}: #1={c1}, #0={c0}, p(1|{c})={p1:.3f}, p(0|{c})={p0:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2589b4",
   "metadata": {},
   "source": [
    "## Pipeline robuste : texte libre → features → prédiction sûre (`safe_predict`)\n",
    "Cette partie gère les **entrées ad hoc** : texte libre ou dictionnaires imparfaits.\n",
    "- Normalisation de texte (`normalize_text`)\n",
    "- Règles simples de détection de mots‑clés (`KEYWORD_RULES`)\n",
    "- Vectorisation (`text_to_features`)\n",
    "- Validation de vecteurs (`validate_instance`)\n",
    "- Prédiction avec **abstention** possible (`safe_predict`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90997eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\")\n",
    "    s = re.sub(r\"[^a-z0-9\\s]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "KEYWORD_RULES = {\n",
    "    \"pas_cher\": [r\"\\bpas\\s*cher\\b\", r\"\\bprix\\b\", r\"\\bbon\\s*plan\\b\", r\"\\breduction\\b\", r\"\\bpromo\\b\"],\n",
    "    \"anglais\":  [r\"\\banglais\\b\", r\"\\btraduction\\b\", r\"\\benglish\\b\", r\"\\btranslate\\b\"],\n",
    "}\n",
    "compiled_rules = {feat: [re.compile(pat) for pat in pats] for feat, pats in KEYWORD_RULES.items()}\n",
    "\n",
    "def text_to_features(query: str, expected_features=(\"pas_cher\",\"anglais\")) -> Dict[str, int]:\n",
    "    q = normalize_text(query)\n",
    "    X = {f: 0 for f in expected_features}\n",
    "    for feat in expected_features:\n",
    "        if feat in compiled_rules:\n",
    "            X[feat] = int(any(pat.search(q) for pat in compiled_rules[feat]))\n",
    "    return X\n",
    "\n",
    "def validate_instance(x: Dict[str, int], expected_features=(\"pas_cher\",\"anglais\")) -> Tuple[bool, str]:\n",
    "    if set(x.keys()) != set(expected_features):\n",
    "        extra = set(x.keys()) - set(expected_features)\n",
    "        missing = set(expected_features) - set(x.keys())\n",
    "        return False, f\"Features inattendues: {sorted(extra)} ; manquantes: {sorted(missing)}\"\n",
    "    for f in expected_features:\n",
    "        if x[f] not in (0,1):\n",
    "            return False, f\"Feature {f} doit valoir 0 ou 1 (reçu: {x[f]!r})\"\n",
    "    return True, \"OK\"\n",
    "\n",
    "def confidence_from_proba(proba: Dict[str, float]) -> float:\n",
    "    vals = sorted(proba.values(), reverse=True)\n",
    "    if len(vals) < 2:\n",
    "        return 1.0\n",
    "    return vals[0] - vals[1]\n",
    "\n",
    "def safe_predict(nb_model, query_or_dict, expected_features=(\"pas_cher\",\"anglais\"), threshold=0.15):\n",
    "    if isinstance(query_or_dict, str):\n",
    "        x = text_to_features(query_or_dict, expected_features)\n",
    "        source = \"text\"\n",
    "    else:\n",
    "        x = dict(query_or_dict)\n",
    "        source = \"dict\"\n",
    "        ok, msg = validate_instance(x, expected_features)\n",
    "        if not ok:\n",
    "            return {\"status\":\"INVALID\", \"reason\":msg, \"features\":x}\n",
    "    proba = nb_model.predict_proba(x)\n",
    "    yhat  = max(proba, key=proba.get)\n",
    "    conf  = confidence_from_proba(proba)\n",
    "    if conf < threshold:\n",
    "        return {\"status\":\"ABSTAIN\", \"reason\":f\"marge={conf:.3f} < seuil={threshold:.3f}\", \"features\":x, \"proba\":proba}\n",
    "    return {\"status\":\"OK\", \"prediction\":yhat, \"proba\":proba, \"confidence\":conf, \"features\":x, \"source\":source}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecac19b",
   "metadata": {},
   "source": [
    "## TODO 1 - Charger les données et entraîner le modèle\n",
    "1. Charger `train_pantalon.csv`.\n",
    "2. Entraîner `NaiveBayesBernoulli(alpha=1.0)`.  \n",
    "3. Afficher **comptes** et **probabilités lissées** via `pretty_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"train_pantalon.csv\"\n",
    "X_train, y_train = load_csv_binary(data_path, feature_names=(\"pas_cher\",\"anglais\"))\n",
    "nb = NaiveBayesBernoulli(alpha=1.0).fit(X_train, y_train)\n",
    "pretty_counts(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfebaa2",
   "metadata": {},
   "source": [
    "## TODO 2 - Prédire et comparer (`alpha=1` vs `alpha=0`)\n",
    "Tester sur : `[1,1]`, `[1,0]`, `[0,1]`, `[0,0]`.  \n",
    "**Comparer** les probabilités et la classe prédite avec/sans lissage. **Interpréter** en 4–6 lignes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad0787",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [\n",
    "    {\"pas_cher\":1, \"anglais\":1},\n",
    "    {\"pas_cher\":1, \"anglais\":0},\n",
    "    {\"pas_cher\":0, \"anglais\":1},\n",
    "    {\"pas_cher\":0, \"anglais\":0},\n",
    "]\n",
    "\n",
    "def run_preds(alpha):\n",
    "    nb = NaiveBayesBernoulli(alpha=alpha).fit(X_train, y_train)\n",
    "    out = []\n",
    "    for x in tests:\n",
    "        proba = nb.predict_proba(x)\n",
    "        yhat = nb.predict(x)\n",
    "        out.append((x, {k:round(v,3) for k,v in proba.items()}, yhat))\n",
    "    return out\n",
    "\n",
    "print(\"== Avec lissage alpha=1.0 ==\")\n",
    "for x, proba, yhat in run_preds(1.0):\n",
    "    print(x, \"→\", yhat, \"| proba:\", proba)\n",
    "\n",
    "print(\"\\n== Sans lissage alpha=0.0 ==\")\n",
    "for x, proba, yhat in run_preds(0.0):\n",
    "    print(x, \"→\", yhat, \"| proba:\", proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3788be4",
   "metadata": {},
   "source": [
    "## TODO 3 - Entrées “ad hoc” (texte libre & validation)\n",
    "1. Tester `safe_predict` avec des **requêtes texte** (orthographes et variantes).  \n",
    "2. Tester `safe_predict` avec des **dicts** corrects/incorrects.  \n",
    "3. Ajuster le **seuil** `threshold` (ex. 0.15 → 0.25) et commenter l'effet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dee697",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayesBernoulli(alpha=1.0).fit(X_train, y_train)\n",
    "\n",
    "queries = [\n",
    "    \"pantalon pas cher pour homme\",\n",
    "    \"patron pantalon en anglais svp\",\n",
    "    \"traduction du mot pantalon\",\n",
    "    \"prix d'un pantalon en promo\",\n",
    "    \"je cherche des infos sur pantalon\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    res = safe_predict(nb, q, expected_features=(\"pas_cher\",\"anglais\"), threshold=0.15)\n",
    "    print(q, \"=>\", res)\n",
    "\n",
    "print(\"\\n-- Dict invalide --\")\n",
    "bad = {\"pas_cher\": 2, \"anglais\": \"oui\"}\n",
    "print(safe_predict(nb, bad, threshold=0.15))\n",
    "\n",
    "print(\"\\n-- Dict valide --\")\n",
    "good = {\"pas_cher\": 1, \"anglais\": 0}\n",
    "print(safe_predict(nb, good, threshold=0.15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755ca6cc",
   "metadata": {},
   "source": [
    "## TODO 4 - Rapport court (5–8 lignes)\n",
    "- Hypothèse **naïve** (indépendance conditionnelle **à classe fixée**).  \n",
    "- Pourquoi elle est pratique (et sa **limite**).  \n",
    "- Interpréter l’effet du **lissage** à partir de vos résultats.  \n",
    "- Intérêt de `safe_predict` pour gérer des entrées **bruitées/ad hoc**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e27978",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "Carte des scores `p(OUI | pas_cher, anglais)` pour `alpha=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c85d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "nb = NaiveBayesBernoulli(alpha=1.0).fit(X_train, y_train)\n",
    "grid = [(a,b) for a in [0,1] for b in [0,1]]\n",
    "scores = [ nb.predict_proba({\"pas_cher\":a,\"anglais\":b})[\"OUI\"] for a,b in grid ]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "im = ax.imshow(np.array(scores).reshape(2,2), vmin=0, vmax=1, origin=\"lower\")\n",
    "ax.set_xticks([0,1]); ax.set_xticklabels([\"anglais=0\",\"anglais=1\"])\n",
    "ax.set_yticks([0,1]); ax.set_yticklabels([\"pas_cher=0\",\"pas_cher=1\"])\n",
    "for i,(a,b) in enumerate(grid):\n",
    "    ax.text(b, a, f\"{scores[i]:.2f}\", ha=\"center\", va=\"center\", color=\"w\")\n",
    "ax.set_title(\"p(OUI | pas_cher, anglais) — alpha=1\")\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d2cfbd",
   "metadata": {},
   "source": [
    "\n",
    "# Extension des features & analyses comparatives\n",
    "Nous étendons le modèle en ajoutant **deux nouvelles features** : `populaire` et `promo`.\n",
    "Le flux de travail :\n",
    "1) **Baseline** avec `[\"pas_cher\", \"anglais\"]`  \n",
    "2) **Modèle étendu** avec `[\"pas_cher\",\"anglais\",\"populaire\",\"promo\"]`  \n",
    "3) **Visuels comparatifs** et **TODO d’analyse**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c4811",
   "metadata": {},
   "source": "## Chargement du dataset (train_pantalon.csv)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"train_pantalon.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(df.head(), \"\\n\")\n",
    "print(\"Colonnes présentes :\", list(df.columns))\n",
    "print(\"N =\", len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b16d6a2",
   "metadata": {},
   "source": "## Utilitaires (chargement X/y, comptages lissés, éval simple)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def load_Xy_from_df(df: pd.DataFrame, feature_names=(\"pas_cher\",\"anglais\")):\n",
    "    X_list, y_list = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        X = {f: int(row[f]) for f in feature_names}\n",
    "        y = str(row[\"achat\"]).strip().upper()\n",
    "        X_list.append(X); y_list.append(y)\n",
    "    return X_list, y_list\n",
    "\n",
    "def counts_table(nb):\n",
    "    rows = []\n",
    "    for c in nb.classes_:\n",
    "        for f in nb.features_:\n",
    "            c1 = nb.feature_counts_[c][(f,1)]\n",
    "            c0 = nb.feature_counts_[c][(f,0)]\n",
    "            tot = c1 + c0\n",
    "            p1 = (c1 + nb.alpha) / (tot + 2*nb.alpha)\n",
    "            p0 = (c0 + nb.alpha) / (tot + 2*nb.alpha)\n",
    "            rows.append({\"classe\":c,\"feature\":f,\"#1\":c1,\"#0\":c0,\"total\":tot,\"p(1|classe)\":round(p1,3),\"p(0|classe)\":round(p0,3)})\n",
    "    priors = {c: nb.class_counts_[c]/nb.n_ for c in nb.classes_}\n",
    "    return pd.DataFrame(rows), priors\n",
    "\n",
    "def evaluate_on(df, feature_names):\n",
    "    X, y = load_Xy_from_df(df, feature_names)\n",
    "    nb = NaiveBayesBernoulli(alpha=1.0).fit(X,y)\n",
    "    yhat = [nb.predict(x) for x in X]\n",
    "    acc = np.mean([a==b for a,b in zip(y,yhat)])\n",
    "    labels = sorted(set(y))\n",
    "    conf = pd.DataFrame(0, index=labels, columns=labels)\n",
    "    for yt, yp in zip(y, yhat):\n",
    "        conf.loc[yt, yp] += 1\n",
    "    return nb, acc, conf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece95f22",
   "metadata": {},
   "source": "## Baseline - features = `['pas_cher','anglais']`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_BASE = [\"pas_cher\",\"anglais\"]\n",
    "nb_base, acc_base, conf_base = evaluate_on(df, FEATURES_BASE)\n",
    "tab_base, priors_base = counts_table(nb_base)\n",
    "\n",
    "print(\"Priors (baseline) :\", {k:round(v,3) for k,v in priors_base.items()})\n",
    "print(\"\\nComptages & p(1|classe) lissées (baseline):\")\n",
    "display(tab_base)\n",
    "\n",
    "print(\"\\nConfusion (baseline):\")\n",
    "display(conf_base)\n",
    "print(\"\\nAccuracy (baseline):\", round(acc_base,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e84c23",
   "metadata": {},
   "source": "## Création de `populaire` et `promo` si absents"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b99ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_ext = df.copy()\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "if \"populaire\" not in df_ext.columns:\n",
    "    base = df_ext[\"pas_cher\"].astype(int).values\n",
    "    bruit = rng.binomial(1, 0.25, size=len(df_ext))\n",
    "    df_ext[\"populaire\"] = np.clip(base | bruit, 0, 1)\n",
    "\n",
    "if \"promo\" not in df_ext.columns:\n",
    "    base = df_ext[\"pas_cher\"].astype(int).values\n",
    "    bonus = rng.binomial(1, 0.35, size=len(df_ext))\n",
    "    df_ext[\"promo\"] = np.where(base==1, 1, bonus)\n",
    "\n",
    "print(df_ext.head())\n",
    "print(\"\\nColonnes (étendu) :\", list(df_ext.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3dd5c",
   "metadata": {},
   "source": "## Modèle étendu - features = `['pas_cher','anglais','populaire','promo']`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74adf4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_EXT = [\"pas_cher\",\"anglais\",\"populaire\",\"promo\"]\n",
    "\n",
    "nb_ext, acc_ext, conf_ext = evaluate_on(df_ext, FEATURES_EXT)\n",
    "tab_ext, priors_ext = counts_table(nb_ext)\n",
    "\n",
    "print(\"Priors (étendu) :\", {k:round(v,3) for k,v in priors_ext.items()})\n",
    "print(\"\\nComptages & p(1|classe) lissées (étendu):\")\n",
    "display(tab_ext)\n",
    "\n",
    "print(\"\\nConfusion (étendu):\")\n",
    "display(conf_ext)\n",
    "print(\"\\nAccuracy (étendu):\", round(acc_ext,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54e8da7",
   "metadata": {},
   "source": "## Visuels comparatifs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Accuracy\n",
    "plt.figure()\n",
    "plt.bar([\"Baseline\",\"Étendu\"], [acc_base, acc_ext])\n",
    "plt.title(\"Accuracy : Baseline vs Étendu\")\n",
    "plt.ylim(0,1)\n",
    "for i,val in enumerate([acc_base, acc_ext]):\n",
    "    plt.text(i, val+0.02, f\"{val:.2f}\", ha=\"center\")\n",
    "plt.show()\n",
    "\n",
    "# 2) p(1|classe=OUI) par feature (baseline vs étendu)\n",
    "def p1_given_class(tab, classe=\"OUI\"):\n",
    "    t = tab[tab[\"classe\"]==classe][[\"feature\",\"p(1|classe)\"]].set_index(\"feature\")\n",
    "    return t[\"p(1|classe)\"]\n",
    "\n",
    "p1_base = p1_given_class(tab_base, \"OUI\")\n",
    "p1_ext  = p1_given_class(tab_ext,  \"OUI\")\n",
    "\n",
    "features_all = sorted(set(p1_base.index).union(p1_ext.index))\n",
    "vals_base = [p1_base.get(f, np.nan) for f in features_all]\n",
    "vals_ext  = [p1_ext.get(f,  np.nan) for f in features_all]\n",
    "\n",
    "x = np.arange(len(features_all)); w = 0.35\n",
    "plt.figure()\n",
    "plt.bar(x - w/2, vals_base, width=w, label=\"Baseline\")\n",
    "plt.bar(x + w/2, vals_ext,  width=w, label=\"Étendu\")\n",
    "plt.xticks(x, features_all, rotation=0)\n",
    "plt.title(\"p(1 | classe=OUI) par feature\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n",
    "\n",
    "# 3) Matrices de confusion (texte)\n",
    "def plot_confusion_text(conf, title):\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    ax.axis(\"off\")\n",
    "    txt = title + \"\\n\\n\" + conf.to_string()\n",
    "    plt.text(0.01, 0.99, txt, va=\"top\", family=\"monospace\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_text(conf_base, \"Confusion — Baseline\")\n",
    "plot_confusion_text(conf_ext,  \"Confusion — Étendu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179d08e",
   "metadata": {},
   "source": [
    "\n",
    "## TODO - Analyse à rédiger\n",
    "1) Comparez les priors et les tableaux de **p(1|classe)** (baseline vs étendu) :  \n",
    "   - Quelles features sont les plus informatives pour `OUI` ?  \n",
    "   - L’ajout de `populaire` et `promo` change-t-il l’ordre d’importance ?\n",
    "2) Comparez les performances :  \n",
    "   - L’accuracy évolue-t-elle ? Quelles hypothèses pour l’expliquer (corrélation/chevauchement) ?  \n",
    "   - Où se situent les erreurs (regardez la **confusion**) ?\n",
    "3) (Option) Calculez la **marge** moyenne (différence entre proba top‑1 et top‑2) avant et après l’extension et commentez l’effet sur la **confiance**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
